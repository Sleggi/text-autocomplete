{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea028ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Импортируем необходимые библиотеки и функции для работы с LSTM и предобученным трансформером:\n",
    "- os, datetime: работа с файловой системой и временем\n",
    "- torch, nn, optim: PyTorch и нейронные сети\n",
    "- Tokenizer: токенизация текста\n",
    "- get_all_dataloaders: загрузка датасетов для LSTM\n",
    "- LSTMModel: модель LSTM\n",
    "- train_one_epoch: обучение LSTM по эпохам\n",
    "- lstm_evaluate, lstm_generate: оценка LSTM и генерация примеров\n",
    "- transformer_evaluate, transformer_generate: генерация и оценка GPT-2\n",
    "\"\"\"\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from src.next_token_dataset import get_all_dataloaders\n",
    "from src.lstm_model import LSTMModel   \n",
    "from src.lstm_train import train_one_epoch\n",
    "from src.lstm_eval import lstm_evaluate, lstm_generate\n",
    "from src.transformer_pipiline_eval import transformer_evaluate, transformer_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed6fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используем device: cpu\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Определяем устройство для вычислений: GPU (CUDA), если доступен, иначе CPU.\n",
    "Выводим, какое устройство используется для обучения и генерации.\n",
    "\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Используем device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d350edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Задаём основные гиперпараметры модели и обучения:\n",
    "- vocab_size: размер словаря для эмбеддингов;\n",
    "- batch_size: размер батча для загрузчика данных;\n",
    "- embedding_dim: размерность векторного представления слов;\n",
    "- hidden_dim: количество скрытых нейронов в LSTM;\n",
    "- num_layers: число слоёв LSTM;\n",
    "- learning_rate: скорость обучения оптимизатора;\n",
    "- dropout: вероятность отключения нейронов для регуляризации;\n",
    "- epochs: количество эпох обучения.\n",
    "\"\"\"\n",
    "vocab_size = 20000\n",
    "batch_size = 256\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "num_layers = 1\n",
    "learning_rate = 1e-3\n",
    "dropout = 0.1\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892d34dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Создаём загрузчики данных для обучения, валидации и тестирования.\n",
    "\n",
    "train_loader — для обучения модели,\n",
    "val_loader — для вычисления метрик на валидационном наборе,\n",
    "test_loader — для оценки на тестовом наборе после обучения.\n",
    "\"\"\"\n",
    "train_path = 'data/train.txt'\n",
    "val_path = 'data/val.txt'\n",
    "test_path = 'data/test.txt'\n",
    "\n",
    "train_loader, val_loader, test_loader = get_all_dataloaders(train_path, \n",
    "                                                            val_path, \n",
    "                                                            test_path, \n",
    "                                                            batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a78962",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Загружаем BPE-токенизатор из файла и создаём словарь idx2word.\n",
    "Этот словарь позволяет переводить числовые токены обратно в текстовые токены\n",
    "\"\"\"\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer/bpe_tokenizer.json\")\n",
    "idx2word = {i: tokenizer.id_to_token(i) for i in range(tokenizer.get_vocab_size())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a96d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Создаём экземпляр LSTM модели с заданными гиперпараметрами (размер словаря, размер эмбеддингов, \n",
    "размер скрытого состояния, количество слоёв и dropout) и переносим модель на выбранное устройство.\n",
    "\"\"\"\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c14a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Определяем функцию потерь для задачи классификации токенов (CrossEntropyLoss) \n",
    "и оптимизатор Adam для обновления весов модели с заданной скоростью обучения.\n",
    "\"\"\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17abb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Функция train_model выполняет полный цикл обучения LSTM-модели:\n",
    "1. Создает уникальную папку для сохранения весов текущего запуска.\n",
    "2. Для каждой эпохи:\n",
    "   a) Обучает модель на тренировочном датасете и выводит loss.\n",
    "   b) Валидирует модель на валидационном датасете, вычисляет loss и ROUGE-1 метрику.\n",
    "   c) Сохраняет веса модели для текущей эпохи.\n",
    "   d) Генерирует примеры текстов (три твита) для оценки качества генерации.\n",
    "3. Использует токенизатор для преобразования текста в токены и обратно.\n",
    "\"\"\"\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device, idx2word=None):\n",
    "    timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    run_folder = os.path.join(\"models\", f\"run_{timestamp}\")\n",
    "    os.makedirs(run_folder, exist_ok=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # обучение\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}\", flush=True)\n",
    "\n",
    "        # валидация\n",
    "        val_loss, rouge_score = lstm_evaluate(model, val_loader, criterion, device, idx2word)\n",
    "        print(f\"Val Loss: {val_loss:.4f}, ROUGE: {rouge_score:.4f}\", flush=True)\n",
    "\n",
    "        # сохраняем веса\n",
    "        save_path = os.path.join(run_folder, f\"lstm_model_epoch{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        # генерируем пример твита\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            example_texts = [\n",
    "                \"The weather today is amazing\",\n",
    "                \"I just watched a movie, it was\",\n",
    "                \"Learning machine learning is fun and exciting\",\n",
    "            ]\n",
    "\n",
    "            for i, text in enumerate(example_texts):\n",
    "                # Токенизируем\n",
    "                encoding = tokenizer.encode(text)\n",
    "                prompt_tokens = torch.tensor([encoding.ids], device=device)\n",
    "\n",
    "                # Создаем последовательность\n",
    "                generated = model.generate(prompt_tokens, max_len=4)\n",
    "\n",
    "                # Декодируем\n",
    "                decoded_text = tokenizer.decode(generated[0].tolist(), skip_special_tokens=True)\n",
    "\n",
    "                print(f\"Tweet {i+1} generated after epoch {epoch+1}: {decoded_text}\\n\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a24a43",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43midx2word\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs, device, idx2word)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_model\u001b[39m(model, train_loader, val_loader, criterion, optimizer, epochs, device, idx2word\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m# обучение\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# валидация\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Personal/ml/dl-learning/dl_cv_course_practicum/sprint_2_rnn/sprint_2_project/text-autocomplete/src/lstm_train.py:6\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m----> 6\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     x, y, mask \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      8\u001b[0m     x, y, mask \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device), mask\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/dl-python311/lib/python3.11/site-packages/torch/_compile.py:51\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     49\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/dl-python311/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/dl-python311/lib/python3.11/site-packages/torch/optim/optimizer.py:967\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[0;32m--> 967\u001b[0m         p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    969\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mgrad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Запуск обучения модели\n",
    "\"\"\"\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epochs,\n",
    "    device,\n",
    "    idx2word\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3585e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Загрузка сохраненных весов обученной LSTM-модели:\n",
    "1. weights_path — путь к файлу с весами модели последней эпохи.\n",
    "2. load_state_dict загружает веса в модель на указанное устройство (CPU или GPU).\n",
    "3. model.eval() переводит модель в режим инференса (отключает dropout и другие тренировочные механизмы).\n",
    "\"\"\"\n",
    "weights_path = \"models/run_2025_08_20_06_59_34/lstm_model_epoch10.pt\"\n",
    "model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c53bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Текст для геренации lstm и transformer для сравнения\n",
    "\"\"\"\n",
    "example_texts = [\n",
    "    \"The weather today is amazing\",\n",
    "    \"I just watched a movie, it was\",\n",
    "    \"Learning machine learning is fun and exciting\",\n",
    "]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b9c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Генерация текста на примере нескольких твитов с использованием:\n",
    "1. lstm_generate — генерация продолжений с обученной LSTM-моделью.\n",
    "2. transformer_generate — генерация продолжений с предобученной моделью DistilGPT-2.\n",
    "Используются одинаковые примеры, чтобы визуально сравнить качество генерации.\n",
    "\"\"\"\n",
    "lstm_generate(example_texts, tokenizer, model, device)\n",
    "transformer_generate(example_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400edbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Валидация предобученной модели DistilGPT-2 на всём датасете:\n",
    "1. Загружаются строки из raw_dataset.txt.\n",
    "2. Для каждой строки модель генерирует продолжение.\n",
    "3. Вычисляется среднее значение метрики ROUGE-1 для всех сгенерированных текстов.\n",
    "Это позволяет сравнить качество генерации GPT-2 обученной LSTM-моделью.\n",
    "\"\"\"\n",
    "with open(\"data/raw_dataset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    val_texts = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "avg_rouge1 = transformer_evaluate(val_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1818e259",
   "metadata": {},
   "source": [
    "### Сравнение LSTM и предобученного трансформера (DistilGPT2)\n",
    "\n",
    "#### 1. LSTM: метрики по эпохам\n",
    "\n",
    "| Эпоха | Train Loss | Val Loss | ROUGE-1 |\n",
    "|-------|------------|----------|---------|\n",
    "| 1/10  | 6.7318     | 6.0905   | 0.1250  |\n",
    "| 2/10  | 6.0621     | 5.9105   | 0.1315  |\n",
    "| 3/10  | 6.0109     | 5.8629   | 0.1366  |\n",
    "| 4/10  | 5.9073     | 5.8732   | 0.1410  |\n",
    "| 5/10  | 5.9176     | 5.9312   | 0.1466  |\n",
    "| 6/10  | 6.0484     | 5.9877   | 0.1503  |\n",
    "| 7/10  | 6.0137     | 6.0257   | 0.1497  |\n",
    "| 8/10  | 6.0844     | 6.0691   | 0.1551  |\n",
    "| 9/10  | 6.2463     | 6.1460   | 0.1492  |\n",
    "| 10/10 | 6.2086     | 6.1846   | 0.1544  |\n",
    "\n",
    "**Примеры генерации LSTM после 10-й эпохи:**\n",
    "\n",
    "- Prompt: *The weather today is amazing*  \n",
    "  Generated: *the weather today is amazing that ' s , sad that has the eee had*\n",
    "\n",
    "- Prompt: *I just watched a movie, it was*  \n",
    "  Generated: *i just watched a movie , it was great do you have at ' ll to self and*\n",
    "\n",
    "- Prompt: *Learning machine learning is fun and exciting*  \n",
    "  Generated: *learning machine learning is fun and exciting will morning , won ' t some on my little*\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Предобученный трансформер DistilGPT2\n",
    "\n",
    "**Примеры генерации на тех же промптах:**\n",
    "\n",
    "- Prompt: *The weather today is amazing*  \n",
    "  Generated: *The weather today is amazing. Weather data from the National Weather Service is available*\n",
    "\n",
    "- Prompt: *I just watched a movie, it was*  \n",
    "  Generated: *I just watched a movie, it was a lot of fun,\" he said. \"I*\n",
    "\n",
    "- Prompt: *Learning machine learning is fun and exciting*  \n",
    "  Generated: *Learning machine learning is fun and exciting. It›s about teaching a new way*\n",
    "\n",
    "**Примеры генерации на raw датасете твитов:**\n",
    "\n",
    "- Prefix: *is upset that he can’t update his Facebook by texting it… and might cry as a resu*  \n",
    "  Target: *lt  School today also. Blah!*  \n",
    "  Generated: *ptor, but he’s still not sure how to respond to this.*  \n",
    "\n",
    "- Prefix: *@Kenichan I dived many times for the ball. Managed to save 50%  Th*  \n",
    "  Target: *e rest go out of bounds*  \n",
    "  Generated: *umbs down from the field during the first half. Can’t get too close to the ball.*  \n",
    "\n",
    "**Среднее значение ROUGE-1 на валидационном датасете:** `0.0476`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Вывод\n",
    "\n",
    "- LSTM показывает **лучший ROUGE-1 (≈0.15)** на валидационном наборе, чем предобученный DistilGPT2 (≈0.048) на тех же данных.  \n",
    "- LSTM генерирует менее «связные» предложения по смыслу, но ближе к специфике тренировочного датасета.  \n",
    "- Предобученный трансформер генерирует грамматически корректные и более «естественные» тексты, но не адаптирован к конкретной задаче автодополнения твитов.\n",
    "\n",
    "> Примечания:\n",
    "> - LSTM обучался на 10 эпохах на конкретном датасете твитов, поэтому его генерации более приближены к стилю тренировочных данных.  \n",
    "> - Предобученный DistilGPT2 не дообучался на этом датасете, поэтому его генерации естественные, но менее похожи на конкретный набор данных.  \n",
    "> - ROUGE-1 измеряет совпадение токенов с целевыми текстами; LSTM показывает выше метрику, потому что «учился» именно на этих твитах.  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
